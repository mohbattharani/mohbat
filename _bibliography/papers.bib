---
---
@string{aps = {American Physical Society,}}

@article{mohbat-cikm-24,
  title={LLaVA-Chef: A Multi-modal Generative Model for Food Recipes},
  author={Fnu Mohbat, Mohammed J. Zaki},  
  journal={Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM)}, 
  volume={},
  pages={},
  year={2024},
  publisher={ACM},
  abstract={In the rapidly evolving landscape of online recipe sharing within a globalized context, there has been a notable surge in research towards comprehending and generating food recipes. Recent advancements in large language models (LLMs) like GPT-2 and LLaVA have paved the way for Natural Language Processing (NLP) approaches to delve deeper into various facets of food-related tasks, encompassing ingredient recognition and comprehensive recipe generation. Despite impressive performance and multi-modal adaptability of LLMs, domain-specific training remains paramount for their effective application. This work evaluates existing LLMs for recipe generation and proposes LLaVA-Chef, a novel model trained on a curated dataset of diverse recipe prompts in a multi-stage approach. First, we refine the mapping of visual food image embeddings to the language space. Second, we adapt LLaVA to the food domain by fine-tuning it on relevant recipe data. Third, we utilize diverse prompts to enhance the model's recipe comprehension. Finally, we improve the linguistic quality of generated recipes by penalizing the model with a custom loss function. LLaVA-Chef demonstrates impressive improvements over pretrained LLMs and prior works. A detailed qualitative analysis reveals that LLaVA-Chef generates more detailed recipes with precise ingredient mentions, compared to existing approaches.},
  html={https://arxiv.org/pdf/2408.16889},
  doi= {10.1145/3627673.3679562},
  altmetric={false},
  bibtex_show={true},
  selected={true},
  preview = {model_llavachef.png}
}

@article{mohbat-TrustNLP-24,
  title={Beyond Visual Augmentation: Investigating Bias in Multi-Modal Text Generation},
  author={Fnu Mohbat, Vijay Sadashivaiah, Keerthiram Murugesan, Amit Dhurandhar, Ronny Luss and Pin-Yu Chen},
  journal={Fourth Workshop on Trustworthy Natural Language Processing (TrustNLP) at NAACL}, 
  volume={},
  pages={1-14},
  year={2024},
  publisher={ACL},
  abstract={The emergence of several contemporary text-to- image generation models such as DALL-E and Stable Diffusion has demonstrated remarkable proficiency in producing high-quality images. While these generated images have been used to improve text quality in natural language gen- eration (NLG) tasks via visual augmentation, parallel research endeavors have found biases within these generated images. Conversely, image-to-text models, grounded in large lan- guage models (LLMs), excel in crafting vivid descriptions of images using high-quality lan- guage, albeit inheriting the biases inherent in LLMs. This research explores how these biases are amplified when generated images are used as input for image-to-text generation models. Through empirical analysis, we show that by feeding biased images into image-to-text mod- els, the generated response becomes even more biased.},
  html={https://trustnlpworkshop.github.io},
  pdf={},
  code={},
  doi= {},
  altmetric={false},
  bibtex_show={true},
  selected={true},
  preview = {frc_gif2023.png}
}

@article{mohbat-acl-24,
  title={GVdoc - Graph-based Visual DOcument Classification},
  author={Mohbat, Fnu and Zaki, Mohammed J. and Finegan{-}Dollak, Catherine and Verma, Ashish},
  journal={Findings of the Association for Computational Linguistics}, 
  volume={},
  pages={5342--5357},
  year={2023},
  publisher={ACL},
  html={https://aclanthology.org/2023.findings-acl.329/},
  pdf={https://aclanthology.org/2023.findings-acl.329.pdf},
  abstract={The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization problem in visual documents due to their diverse layouts. They also require a lot of computing power during inference, making them impractical for many real-world applications. We propose, GVdoc, a graph-based document classification model that addresses both of these challenges. Our approach generates a document graph based on its layout, and then trains a graph neural network to learn node and graph embeddings. Through experiments, we show that our model, even with fewer parameters, outperforms state-of-the-art models on out-of-distribution data while retaining comparable performance on the in-distribution test set.},
  bibtex_show={true},
  altmetric={false},
  selected={true},
  preview = {mujtaba2024ff.png}
}

@article{ahmad2023education,
  title={Education 5.0: requirements, enabling technologies, and future directions},
  author={Ahmad, Shabir and Umirzakova, Sabina and Mujtaba, Ghulam and Amin, Muhammad Sadiq and Whangbo, Taegkeun},
  abstract={},
  html={https://arxiv.org/abs/2307.15846},
  pdf={https://arxiv.org/pdf/2307.15846},
  journal={arXiv preprint arXiv:2307.15846},
  doi= {10.48550/arXiv.2307.15846},
  year={2023},
  altmetric={false},
  bibtex_show={true},
  preview = {ahmad2023education.png}
}
